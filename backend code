# 1. Load packages
library(tidyverse)
library(caret)
library(xgboost)
library(smotefamily)
library(Matrix)
library(ggplot2)
library(pROC)
library(reshape2)

# 2. Load and clean data
file_path <- file.choose()
data <- read.csv(file_path)

colnames(data)[which(names(data) == "Stress_Category")] <- "Stress_Level"
data <- na.omit(data)
data <- data %>% mutate_if(is.character, as.factor)

# 3. Drop weak columns (optional tuning step)
data <- data %>% select(-Occupation, -Recent.Major.Life.Event, -Medication)

# 4. Encode factors
target <- as.factor(data$Stress_Level)
data$Stress_Level <- NULL

# One-hot encode
data_matrix <- model.matrix(~ ., data = data)[, -1]

# 5. Combine with labels for SMOTE
data_full <- data.frame(data_matrix, label = target)

# 6. Apply multi-class SMOTE
set.seed(123)
smote_result <- SMOTE(data_full[, -ncol(data_full)], data_full$label, K = 5)
balanced_data <- smote_result$data
balanced_data$label <- as.factor(balanced_data$class)
balanced_data$class <- NULL

# 7. Train-test split
set.seed(123)
train_idx <- createDataPartition(balanced_data$label, p = 0.8, list = FALSE)
train <- balanced_data[train_idx, ]
test <- balanced_data[-train_idx, ]

dtrain <- xgb.DMatrix(data = as.matrix(train[, -ncol(train)]), label = as.numeric(train$label) - 1)
dtest  <- xgb.DMatrix(data = as.matrix(test[, -ncol(test)]),  label = as.numeric(test$label) - 1)

# 8. Train XGBoost
params <- list(
  objective = "multi:softmax",
  num_class = length(levels(target)),
  eval_metric = "merror",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

model <- xgb.train(params, dtrain, nrounds = 200, watchlist = list(val = dtest), verbose = 0)

# 9. Predict and evaluate
preds <- predict(model, dtest)
accuracy <- mean(preds == as.numeric(test$label) - 1)
cat("âœ… Final Accuracy after SMOTE + XGBoost:", round(accuracy * 100, 2), "%\n")

# 10. Add suggestions
label_levels <- levels(target)
predicted_labels <- factor(label_levels[preds + 1], levels = label_levels)

suggestions <- sapply(predicted_labels, function(level) {
  switch(level,
         "High" = "Daily meditation, reduce caffeine, seek professional help.",
         "Medium" = "Exercise regularly, maintain sleep hygiene.",
         "Low" = "Continue healthy habits and self-awareness.")
})

results <- data.frame(
  Actual = test$label,
  Predicted = predicted_labels,
  Suggestion = suggestions
)

# Show first few rows
print(head(results, 10))

# === 11. Confusion Matrix and Stats ===
cat("\nðŸ“Š Confusion Matrix and Class Metrics:\n")
conf_mat <- confusionMatrix(predicted_labels, test$label)
print(conf_mat)

# Extract F1, Precision, Recall
cat("\nðŸ“ˆ Precision, Recall, and F1-Score:\n")
metrics <- data.frame(conf_mat$byClass)
print(metrics[, c("Precision", "Recall", "F1")])

# === 12. Visualization ===

# Bar Plot of Predictions
ggplot(results, aes(x = Predicted, fill = Predicted)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Predicted Stress Levels", x = "Stress Level", y = "Count")

# Heatmap of Confusion Matrix
cm <- table(results$Actual, results$Predicted)
cm_df <- as.data.frame(cm)
colnames(cm_df) <- c("Actual", "Predicted", "Freq")

ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1.5, color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Confusion Matrix Heatmap") +
  theme_minimal()

# === 13. ROC Curve (Optional, use only with softprob output) ===
# If you switch to "multi:softprob" instead of "softmax", you can uncomment this part:

# prob_preds <- predict(model, dtest)
# prob_matrix <- matrix(prob_preds, ncol = length(label_levels), byrow = TRUE)
# colnames(prob_matrix) <- label_levels
# 
# for (lvl in label_levels) {
#   true_bin <- as.numeric(test$label == lvl)
#   roc_curve <- roc(true_bin, prob_matrix[, lvl])
#   cat(sprintf("AUC for %s: %.3f\n", lvl, auc(roc_curve)))
#   plot(roc_curve, main = paste("ROC Curve -", lvl))
# }
